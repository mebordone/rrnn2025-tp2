## Resumen

Este trabajo presenta la implementación y análisis de una red neuronal feedforward multicapa para la clasificación del dataset Fashion-MNIST utilizando PyTorch. El objetivo principal fue explorar el impacto de diferentes hiperparámetros en el rendimiento del modelo mediante un proceso sistemático de experimentación. Se evaluaron variaciones en la tasa de aprendizaje (0.0001, 0.001, 0.01), optimizadores (SGD y ADAM), valores de dropout (0.0, 0.2, 0.4, 0.6), arquitecturas de red (64-32, 128-64, 256-128, 512-256), número de épocas (5-30) y tamaños de batch (32-256). Los resultados demostraron que el optimizador ADAM supera significativamente a SGD, alcanzando 88.06% de precisión en validación. La configuración óptima final, combinando ADAM con learning rate 0.001, arquitectura 256-128, batch size 32, dropout 0.2 y 30 épocas, logró una precisión de **87.99%** en el conjunto de validación. El análisis reveló que batch sizes pequeños (32) y arquitecturas moderadas (256-128) ofrecen el mejor balance entre rendimiento y eficiencia computacional, mientras que el dropout de 0.2-0.4 proporciona una regularización efectiva sin comprometer la capacidad de aprendizaje del modelo.

